{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import libs.functions as _fn\n",
    "from libs.data import Dataset\n",
    "import libs.losses as _losses\n",
    "import libs.classifier as _cls\n",
    "import libs.models3 as models\n",
    "import wandb\n",
    "import os\n",
    "import torchinfo\n",
    "import sys\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "_fn.fix_seed(1)\n",
    "RESUME  = False\n",
    "WANDB   = False\n",
    "FREEZE  = False\n",
    "NOTRAIN = False\n",
    "cfg     = 'timit'\n",
    "CONFIG = _fn.load_yaml(cfg)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(CONFIG['general']['gpu_id']['value'])\n",
    "\n",
    "if RESUME:\n",
    "    checkpoint = torch.load(CONFIG['general']['checkpoint']['value'])\n",
    "\n",
    "if WANDB:\n",
    "    if RESUME:\n",
    "        checkpoint_file = CONFIG['general']['checkpoint']['value']\n",
    "        RUN_ID = os.path.basename(checkpoint_file).rstrip('.dict')[:-3]\n",
    "        print(f'Your run id is {RUN_ID} with checkpoint {checkpoint_file}')\n",
    "        input(\"Press any key if you want to continue >>\")\n",
    "        wprj = wandb.init(id=RUN_ID,\n",
    "                          project=CONFIG['wandb']['project']['value'],\n",
    "                          resume=\"must\",\n",
    "                          config=CONFIG)\n",
    "    else:\n",
    "        wprj = wandb.init(project=CONFIG['wandb']['project']['value'],\n",
    "                          resume=False,\n",
    "                          config=CONFIG)\n",
    "        RUN_ID = wprj.id\n",
    "else:\n",
    "    RUN_ID = _fn.get_random_hash()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "_fn.report(\"Torch is using device:\", device)\n",
    "\n",
    "Model = getattr(models, CONFIG['model']['name']['value'])\n",
    "model = Model(num_classes=CONFIG['general']['classes']['value'])\n",
    "\n",
    "model.float()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "_fn.report(f\"Model {CONFIG['model']['name']['value']} created\")\n",
    "\n",
    "if RESUME:\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    _fn.report(\"Model state dict loaded from checkpoint\")\n",
    "if FREEZE:\n",
    "    if not RESUME:\n",
    "        raise Exception('Process started anew, cannot freeze new layers')\n",
    "    if model.extractor:\n",
    "        for param in model.extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        _fn.report('Model Extractor block parameters are frozen')\n",
    "    else:\n",
    "        raise Exception('No \\'extractor\\' block in model')\n",
    "\n",
    "print(model)\n",
    "torchinfo.summary(model, tuple(CONFIG['general']['torchinfo_shape']['value']))\n",
    "\n",
    "# Setting up initial epoch\n",
    "initial_epoch = 0\n",
    "if RESUME:\n",
    "    initial_epoch = int(checkpoint['epoch']) + 1\n",
    "    _fn.report(\"Initial epoch set to\", initial_epoch)\n",
    "\n",
    "# Setting up criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up optimizer and scheduler\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['optimizer']['initial_lr']['value'],\n",
    "    momentum=CONFIG['optimizer']['momentum']['value'],\n",
    "    weight_decay=CONFIG['optimizer']['weight_decay']['value'],\n",
    "    nesterov=CONFIG['optimizer']['nesterov']['value'])\n",
    "#if RESUME:\n",
    "#    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#    _fn.report(\"Optimizer state dict loaded from checkpoint\")\n",
    "_fn.report(\"Optimizer initialized\")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "#scheduler = StepDownScheduler(optimizer,\n",
    "#                              initial_epoch=initial_epoch,\n",
    "#                              config=CONFIG['scheduler'])\n",
    "#_fn.report(\"Scheduler initialized\")\n",
    "\n",
    "# Setting up datasets and data loaders\n",
    "D = Dataset(filename=CONFIG['dataset']['train']['file']['value'],\n",
    "            cache_paths=CONFIG['dataset']['train']['dirs']['value'],\n",
    "            force_even=True,\n",
    "            useindex=CONFIG['dataset']['useindex']['value'])\n",
    "DT = D.get_randomized_subset_with_augmentation(\n",
    "    max_records=20,\n",
    "    speakers_filter=D.get_unique_speakers(),\n",
    "    augmentations_filter=[],\n",
    "    useindex=CONFIG['dataset']['useindex']['value'])\n",
    "T = Dataset(filename=CONFIG['dataset']['test']['file']['value'],\n",
    "            cache_paths=CONFIG['dataset']['test']['dirs']['value'],\n",
    "            useindex=CONFIG['dataset']['useindex']['value'])\n",
    "\n",
    "train_loader = DataLoader(D,\n",
    "                          batch_size=CONFIG['general']['batch_size']['value'],\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "test_loader = DataLoader(T,\n",
    "                         batch_size=CONFIG['general']['batch_size']['value'],\n",
    "                         shuffle=False,\n",
    "                         num_workers=2)\n",
    "train_eval_loader = DataLoader(\n",
    "    DT,\n",
    "    batch_size=CONFIG['general']['batch_size']['value'],\n",
    "    shuffle=False,\n",
    "    num_workers=0)\n",
    "_fn.report(\"Datasets loaded\")\n",
    "\n",
    "########################################################\n",
    "####                  NN cycle                      ####\n",
    "########################################################\n",
    "if WANDB:\n",
    "    wandb.watch(model)\n",
    "\n",
    "top1 = checkpoint['top1'] if RESUME and 'top1' in checkpoint.keys() else 0\n",
    "lss = _losses.Losses()\n",
    "for epoch in range(initial_epoch, CONFIG['general']['epochs']['value']):\n",
    "    _fn.report(\"**************** Epoch\", epoch, \"out of\",\n",
    "               CONFIG['general']['epochs']['value'], \"****************\")\n",
    "    lss = _losses.Losses()\n",
    "    _fn.report(\"-------------- Training ----------------\")\n",
    "\n",
    "    if not NOTRAIN:\n",
    "        losses = _cls.train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            augmentations=CONFIG['augmentations']['value'],\n",
    "            num_classes=CONFIG['general']['classes']['value'],\n",
    "            extras={},\n",
    "            mode=CONFIG['augmentations']['train_mode']['value'])\n",
    "        lss.append(losses, epoch)\n",
    "        _fn.report(f'Epoch loss: {lss.mean(epoch):.4f}')\n",
    "    \"\"\"\n",
    "    _fn.report(\"-------------- Visualization ----------------\")\n",
    "    if epoch > 0 and epoch % 1 == 0:\n",
    "        dataset = _db.visualize(D, epoch, samples=30)\n",
    "    \"\"\"\n",
    "\n",
    "    top1train, top5train, top1test, top5test, test_loss = _cls.test(\n",
    "        train_eval_loader, test_loader, model, criterion)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(\n",
    "        f\"T1train: {top1train}, T5train: {top5train}, T1test: {top1test}, T5test: {top5test}, lr: {current_lr:.4f}\"\n",
    "    )\n",
    "\n",
    "    if WANDB:\n",
    "        wandb.log({\n",
    "            \"Loss\": lss.mean(epoch),\n",
    "            \"Test loss\": test_loss,\n",
    "            \"Top1 acc over training data\": top1train,\n",
    "            \"Top5 acc over training data\": top5train,\n",
    "            \"Top1 acc over test data\": top1test,\n",
    "            \"Top5 acc over test data\": top5test,\n",
    "            \"Learning rate\": current_lr\n",
    "        })\n",
    "\n",
    "    ##########################################################\n",
    "    ##### Saving checkpoint if testing accuracy improved\n",
    "    ##########################################################\n",
    "    if top1test > top1:\n",
    "        top1 = top1test\n",
    "        _fn.checkpoint(id=RUN_ID,\n",
    "                       data={\n",
    "                           'epoch': epoch,\n",
    "                           'state_dict': model.state_dict(),\n",
    "                           'optimizer': optimizer.state_dict(),\n",
    "                           'top1': top1\n",
    "                       })\n",
    "    #if _fn.early_stop(lss.mean_per_epoch(), criterion='min'):\n",
    "    #    print(\"Early stop triggered\")\n",
    "    #    sys.exit(0)\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4 | packaged by conda-forge | (default, May 10 2021, 22:10:52) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fb7518a2e8a17b2a6f616f71a2a682ef6431f70cbc8f460034aa67a597afd09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
